import cv2
import mediapipe as mp
from scipy.spatial import distance
import numpy as np
from playsound import playsound
import threading
import math # C·∫ßn cho t√≠nh to√°n g√≥c

# --- Kh·ªüi t·∫°o Mediapipe ---
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# --- C√°c h·∫±ng s·ªë cho Landmarks ---
# M·∫Øt
LEFT_EYE = [33, 160, 158, 133, 153, 144]
RIGHT_EYE = [362, 385, 387, 263, 373, 380]
# Mi·ªáng (cho nh·∫≠n di·ªán ng√°p)
MOUTH_OUTER = [61, 291, 0, 17] # Tr√°i, Ph·∫£i, Tr√™n, D∆∞·ªõi (ch·ªâ ƒë·ªÉ tham kh·∫£o)
MOUTH_INNER = [78, 308, 13, 14] # T∆∞∆°ng t·ª±, cho MAR
# ƒêi·ªÉm landmarks cho t∆∞ th·∫ø ƒë·∫ßu (Head Pose)
HEAD_POSE_LANDMARKS = [
    33, 263, 1, 61, 291, 199 # M·∫Øt tr√°i, M·∫Øt ph·∫£i, M≈©i, Mi·ªáng tr√°i, Mi·ªáng ph·∫£i, C·∫±m
]

# --- C√°c h√†m t√≠nh to√°n ---

# H√†m t√≠nh Eye Aspect Ratio (EAR)
def eye_aspect_ratio(eye):
    A = distance.euclidean(eye[1], eye[5])
    B = distance.euclidean(eye[2], eye[4])
    C = distance.euclidean(eye[0], eye[3])
    ear = (A + B) / (2.0 * C)
    return ear

# H√†m t√≠nh Mouth Aspect Ratio (MAR)
def mouth_aspect_ratio(mouth):
    # T√≠nh kho·∫£ng c√°ch d·ªçc (m√¥i tr√™n v√† m√¥i d∆∞·ªõi)
    A = distance.euclidean(mouth[0], mouth[1]) # V√≠ d·ª•: (13, 14)
    # T√≠nh kho·∫£ng c√°ch ngang (2 m√©p)
    B = distance.euclidean(mouth[2], mouth[3]) # V√≠ d·ª•: (61, 291)
    if B == 0: # Tr√°nh chia cho 0
        return 0
    mar = A / B
    return mar

# H√†m ph√°t √¢m thanh l·∫∑p l·∫°i
def play_alert_loop(stop_event):
    while not stop_event.is_set():
        playsound("alert.wav")

# --- C√°c ng∆∞·ª°ng (Thresholds) v√† Bi·∫øn to√†n c·ª•c ---

# Ng∆∞·ª°ng EAR
EAR_THRESH = 0.25
EAR_FRAME_CHECK = 20
eye_flag = 0

# Ng∆∞·ª°ng MAR (Ng√°p)
MAR_THRESH = 0.5 # Ng∆∞·ª°ng n√†y c·∫ßn ƒë∆∞·ª£c tinh ch·ªânh
YAWN_FRAME_CHECK = 10
yawn_flag = 0

# Ng∆∞·ª°ng Head Nod (G·∫≠t ƒë·∫ßu)
NOD_PITCH_THRESH = 20 # ƒê·ªô (c√∫i xu·ªëng 20 ƒë·ªô)
NOD_FRAME_CHECK = 15
nod_flag = 0

# Bi·∫øn ki·ªÉm so√°t lu·ªìng √¢m thanh
stop_alert_event = threading.Event()
alert_thread = None

# --- Kh·ªüi ƒë·ªông Camera ---
cap = cv2.VideoCapture(0)
print("üì∏ Nh·∫•n Q ƒë·ªÉ tho√°t...")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # L·∫•y k√≠ch th∆∞·ªõc khung h√¨nh
    h, w, _ = frame.shape

    frame = cv2.flip(frame, 1)
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # X·ª≠ l√Ω frame v·ªõi Mediapipe
    results = face_mesh.process(rgb)

    ear = 0.0 # Kh·ªüi t·∫°o ear
    is_trigger_alert = False # Bi·∫øn ki·ªÉm so√°t vi·ªác k√≠ch ho·∫°t c·∫£nh b√°o

    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            lm = face_landmarks.landmark
            
            # Chuy·ªÉn ƒë·ªïi landmarks sang t·ªça ƒë·ªô pixel (x, y)
            def get_coords(index):
                return (int(lm[index].x * w), int(lm[index].y * h))
            
            # 1. T√çNH TO√ÅN NH·∫ÆM M·∫ÆT (EAR)
            left_eye_coords = [get_coords(i) for i in LEFT_EYE]
            right_eye_coords = [get_coords(i) for i in RIGHT_EYE]
            
            cv2.polylines(frame, [np.array(left_eye_coords, dtype=np.int32)], True, (0,255,0), 1)
            cv2.polylines(frame, [np.array(right_eye_coords, dtype=np.int32)], True, (0,255,0), 1)

            leftEAR = eye_aspect_ratio(left_eye_coords)
            rightEAR = eye_aspect_ratio(right_eye_coords)
            ear = (leftEAR + rightEAR) / 2.0

            # 2. T√çNH TO√ÅN NG√ÅP (MAR)
            # L·∫•y 4 ƒëi·ªÉm: m√¥i tr√™n (13), m√¥i d∆∞·ªõi (14), m√©p tr√°i (61), m√©p ph·∫£i (291)
            mouth_coords = [get_coords(13), get_coords(14), get_coords(61), get_coords(291)]
            mar = mouth_aspect_ratio(mouth_coords)

            # 3. T√çNH TO√ÅN G·∫¨T ƒê·∫¶U (HEAD POSE - PITCH)
            # L·∫•y c√°c ƒëi·ªÉm 2D tr√™n ·∫£nh
            image_points = np.array([
                get_coords(1),    # M≈©i
                get_coords(199),  # C·∫±m
                get_coords(33),   # G√≥c m·∫Øt tr√°i
                get_coords(263),  # G√≥c m·∫Øt ph·∫£i
                get_coords(61),   # M√©p tr√°i
                get_coords(291)   # M√©p ph·∫£i
            ], dtype="double")
            
            # M√¥ h√¨nh 3D (t·ªça ƒë·ªô chu·∫©n)
            model_points = np.array([
                (0.0, 0.0, 0.0),             # M≈©i
                (0.0, -330.0, -65.0),        # C·∫±m
                (-225.0, 170.0, -135.0),     # G√≥c m·∫Øt tr√°i
                (225.0, 170.0, -135.0),      # G√≥c m·∫Øt ph·∫£i
                (-150.0, -150.0, -125.0),    # M√©p tr√°i
                (150.0, -150.0, -125.0)      # M√©p ph·∫£i
            ])
            
            # Th√¥ng s·ªë camera (gi·∫£ ƒë·ªãnh)
            focal_length = w
            center = (w / 2, h / 2)
            camera_matrix = np.array(
                [[focal_length, 0, center[0]],
                 [0, focal_length, center[1]],
                 [0, 0, 1]], dtype="double"
            )
            dist_coeffs = np.zeros((4, 1)) # Kh√¥ng c√≥ m√©o ·ªëng k√≠nh

            # Gi·∫£i PnP ƒë·ªÉ t√¨m t∆∞ th·∫ø
            (success, rotation_vector, translation_vector) = cv2.solvePnP(
                model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE
            )

            #
            # <<< PH·∫¶N S·ª¨A L·ªñI S·ªê 2 B·∫ÆT ƒê·∫¶U T·∫†I ƒê√ÇY >>>
            #
            # Ch·ªâ t√≠nh to√°n g√≥c n·∫øu solvePnP th√†nh c√¥ng
            if success:
                # L·∫•y g√≥c Pitch (c√∫i/ng·ª≠a)
                (rotation_matrix, _) = cv2.Rodrigues(rotation_vector)
                P_mat = np.hstack((rotation_matrix, translation_vector))
                (_, _, _, _, _, _, euler_angles) = cv2.decomposeProjectionMatrix(P_mat)
                pitch = euler_angles[0]
                
                # C·∫≠p nh·∫≠t c·ªù g·∫≠t ƒë·∫ßu
                if pitch > NOD_PITCH_THRESH:
                    nod_flag += 1
                else:
                    nod_flag = 0
            else:
                # N·∫øu PnP th·∫•t b·∫°i, reset c·ªù
                nod_flag = 0
            #
            # <<< K·∫æT TH√öC PH·∫¶N S·ª¨A L·ªñI S·ªê 2 >>>
            #

            # --- LOGIC KI·ªÇM TRA T·ªîNG H·ª¢P ---
            
            # C·∫≠p nh·∫≠t c·ªù (flag) cho m·∫Øt v√† ng√°p
            if ear < EAR_THRESH:
                eye_flag += 1
            else:
                eye_flag = 0
            
            if mar > MAR_THRESH:
                yawn_flag += 1
            else:
                yawn_flag = 0

            # (Logic c·ªù g·∫≠t ƒë·∫ßu ƒë√£ ƒë∆∞·ª£c chuy·ªÉn v√†o kh·ªëi 'if success' ·ªü tr√™n)

            # Ki·ªÉm tra xem c√≥ k√≠ch ho·∫°t c·∫£nh b√°o kh√¥ng
            if eye_flag >= EAR_FRAME_CHECK:
                is_trigger_alert = True
                cv2.putText(frame, "EYES CLOSED", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            if yawn_flag >= YAWN_FRAME_CHECK:
                is_trigger_alert = True
                cv2.putText(frame, "YAWNING", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                yawn_flag = 0 # Reset ƒë·ªÉ c√≥ th·ªÉ ph√°t hi·ªán ng√°p l·∫ßn n·ªØa
            
            if nod_flag >= NOD_FRAME_CHECK:
                is_trigger_alert = True
                cv2.putText(frame, "HEAD NOD", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                nod_flag = 0 # Reset ƒë·ªÉ ph√°t hi·ªán g·∫≠t ƒë·∫ßu l·∫ßn n·ªØa

    else:
        # <<< TH√äM M·ªöI (QUAN TR·ªåNG) >>>
        # N·∫øu kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t, h√£y reset t·∫•t c·∫£ c√°c c·ªù
        # ƒêi·ªÅu n√†y ngƒÉn vi·ªác b√°o ƒë·ªông sai khi khu√¥n m·∫∑t xu·∫•t hi·ªán tr·ªü l·∫°i
        eye_flag = 0
        yawn_flag = 0
        nod_flag = 0


    # --- LOGIC C·∫¢NH B√ÅO (B√äN NGO√ÄI V√íNG L·∫∂P LANDMARKS) ---
    
    # 1. B·∫ÆT ƒê·∫¶U C·∫¢NH B√ÅO
    # N·∫øu b·∫•t k·ª≥ c·ªù n√†o ƒë∆∞·ª£c k√≠ch ho·∫°t (m·∫Øt nh·∫Øm, ng√°p, ho·∫∑c g·∫≠t ƒë·∫ßu)
    if is_trigger_alert:
        cv2.putText(frame, "‚ö†Ô∏è DROWSINESS ALERT!", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # N·∫øu lu·ªìng √¢m thanh ch∆∞a ch·∫°y, h√£y kh·ªüi ƒë·ªông n√≥
        if alert_thread is None or not alert_thread.is_alive():
            stop_alert_event.clear()
            alert_thread = threading.Thread(target=play_alert_loop, 
                                            args=(stop_alert_event,), 
                                            daemon=True)
            alert_thread.start()

    # 2. D·ª™NG C·∫¢NH B√ÅO
    # Ch·ªâ d·ª´ng l·∫°i khi m·∫Øt m·ªü (ear > thresh V√Ä eye_flag ƒë√£ reset v·ªÅ 0)
    # Logic n√†y v·∫´n ƒë√∫ng: n·∫øu kh√¥ng c√≥ m·∫∑t, ear = 0.0, b√°o ƒë·ªông s·∫Ω kh√¥ng d·ª´ng
    if ear >= EAR_THRESH and eye_flag == 0:
        if alert_thread is not None and alert_thread.is_alive():
            stop_alert_event.set()
            alert_thread = None

    # Hi·ªÉn th·ªã frame
    cv2.imshow("Drowsiness Detection v2 (Eyes, Yawn, Nod)", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# D·ªçn d·∫πp tr∆∞·ªõc khi tho√°t
if alert_thread is not None and alert_thread.is_alive():
    stop_alert_event.set()

cap.release()
cv2.destroyAllWindows()