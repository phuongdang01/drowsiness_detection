import cv2
import mediapipe as mp
from scipy.spatial import distance
import numpy as np
from playsound import playsound
import threading
import math
import time
import os # Th√™m th∆∞ vi·ªán OS ƒë·ªÉ ki·ªÉm tra t·ªáp
import sys # TH√äM M·ªöI: C·∫ßn thi·∫øt cho l·ªánh sys.exit()

# --- Bi·∫øn ki·ªÉm so√°t √¢m thanh ---
# Bi·∫øn n√†y s·∫Ω gi√∫p ch√∫ng ta d·ª´ng lu·ªìng √¢m thanh
stop_alert_event = threading.Event()
alert_thread = None
SOUND_FILE = "alert.wav"

# --- Kh·ªüi t·∫°o Mediapipe ---
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)
mp_drawing = mp.solutions.drawing_utils
drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1, color=(0, 255, 0))

# --- C√°c h·∫±ng s·ªë cho Landmarks ---
# M·∫Øt (d·ª±a tr√™n 478 ƒëi·ªÉm c·ªßa Mediapipe)
LEFT_EYE = [33, 160, 158, 133, 153, 144]
RIGHT_EYE = [362, 385, 387, 263, 373, 380]
# Mi·ªáng (cho nh·∫≠n di·ªán ng√°p)
MOUTH_INNER = [13, 14, 61, 291] # M√¥i tr√™n, M√¥i d∆∞·ªõi, M√©p tr√°i, M√©p ph·∫£i
# TH√äM M·ªöI: Vi·ªÅn ngo√†i c·ªßa mi·ªáng (20 ƒëi·ªÉm, t∆∞∆°ng t·ª± dlib)
MOUTH_OUTLINE = [
    61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291, 375, 321, 405, 314, 17, 
    84, 181, 91, 146
]

# --- C√°c h√†m t√≠nh to√°n ---

def eye_aspect_ratio(eye_coords):
    """T√≠nh to√°n EAR d·ª±a tr√™n t·ªça ƒë·ªô (ƒë√£ chu·∫©n h√≥a ho·∫∑c pixel)"""
    # T√≠nh kho·∫£ng c√°ch d·ªçc
    A = distance.euclidean(eye_coords[1], eye_coords[5])
    B = distance.euclidean(eye_coords[2], eye_coords[4])
    # T√≠nh kho·∫£ng c√°ch ngang
    C = distance.euclidean(eye_coords[0], eye_coords[3])
    
    if C == 0:
        return 0
    ear = (A + B) / (2.0 * C)
    return ear

def mouth_aspect_ratio(mouth_coords):
    """T√≠nh to√°n MAR (M√¥i tr√™n, M√¥i d∆∞·ªõi, M√©p tr√°i, M√©p ph·∫£i)"""
    # T√≠nh kho·∫£ng c√°ch d·ªçc
    A = distance.euclidean(mouth_coords[0], mouth_coords[1])
    # T√≠nh kho·∫£ng c√°ch ngang
    B = distance.euclidean(mouth_coords[2], mouth_coords[3])
    if B == 0:
        return 0
    mar = A / B
    return mar

# H√†m ph√°t √¢m thanh l·∫∑p l·∫°i (trong m·ªôt lu·ªìng ri√™ng)
def play_alert_loop(stop_event_check):
    """Ph√°t t·ªáp √¢m thanh l·∫∑p ƒëi l·∫∑p l·∫°i cho ƒë·∫øn khi s·ª± ki·ªán stop_event ƒë∆∞·ª£c set."""
    while not stop_event_check.is_set():
        try:
            playsound(SOUND_FILE)
            time.sleep(0.5) # T·∫°m d·ª´ng ng·∫Øn ƒë·ªÉ tr√°nh ch·ªìng ch√©o √¢m thanh
        except Exception as e:
            if not stop_event_check.is_set():
                print(f"[L·ªñI √ÇM THANH] Kh√¥ng th·ªÉ ph√°t '{SOUND_FILE}': {e}")
                print("Vui l√≤ng ƒë·∫£m b·∫£o t·ªáp t·ªìn t·∫°i v√† th∆∞ vi·ªán 'playsound' ho·∫°t ƒë·ªông.")
            break # Tho√°t kh·ªèi v√≤ng l·∫∑p n·∫øu c√≥ l·ªói

# H√†m qu·∫£n l√Ω lu·ªìng √¢m thanh
def trigger_alert(start_alert=True):
    """K√≠ch ho·∫°t ho·∫∑c d·ª´ng lu·ªìng c·∫£nh b√°o."""
    global alert_thread, stop_alert_event

    if start_alert:
        # 1. B·∫ÆT ƒê·∫¶U C·∫¢NH B√ÅO
        # N·∫øu lu·ªìng ch∆∞a ch·∫°y, h√£y kh·ªüi ƒë·ªông n√≥
        if alert_thread is None or not alert_thread.is_alive():
            print("[C·∫¢NH B√ÅO] K√≠ch ho·∫°t c·∫£nh b√°o ng·ªß g·∫≠t!")
            stop_alert_event.clear()
            alert_thread = threading.Thread(target=play_alert_loop,
                                            args=(stop_alert_event,),
                                            daemon=True)
            alert_thread.start()
    else:
        # 2. D·ª™NG C·∫¢NH B√ÅO
        if alert_thread is not None and alert_thread.is_alive():
            print("[INFO] T·∫Øt c·∫£nh b√°o.")
            stop_alert_event.set()
            alert_thread = None

# --- Ki·ªÉm tra t·ªáp √¢m thanh tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu ---
if not os.path.exists(SOUND_FILE):
    print(f"[L·ªñI] Kh√¥ng t√¨m th·∫•y t·ªáp √¢m thanh: '{SOUND_FILE}'")
    print("Ch∆∞∆°ng tr√¨nh s·∫Ω ch·∫°y m√† kh√¥ng c√≥ √¢m thanh c·∫£nh b√°o.")
    
# --- C√°c ng∆∞·ª°ng (Thresholds) v√† Bi·∫øn to√†n c·ª•c ---

# Ng∆∞·ª°ng EAR (T·ª∑ l·ªá khung m·∫Øt)
EAR_THRESH = 0.25      # Ng∆∞·ª°ng nh·∫Øm m·∫Øt (c·∫ßn tinh ch·ªânh cho camera c·ªßa b·∫°n)
EAR_FRAME_CHECK = 15   # S·ªë khung h√¨nh li√™n ti·∫øp ƒë·ªÉ k√≠ch ho·∫°t
eye_flag_counter = 0

# Ng∆∞·ª°ng MAR (Ng√°p)
MAR_THRESH = 0.5       # Ng∆∞·ª°ng ng√°p (c·∫ßn tinh ch·ªânh)
YAWN_FRAME_CHECK = 20  # S·ªë khung h√¨nh li√™n ti·∫øp ƒë·ªÉ k√≠ch ho·∫°t
yawn_flag_counter = 0

# Ng∆∞·ª°ng Head Nod (G·∫≠t ƒë·∫ßu)
NOD_PITCH_THRESH = 20  # ƒê·ªô (c√∫i xu·ªëng 20 ƒë·ªô so v·ªõi ph∆∞∆°ng ngang)
NOD_FRAME_CHECK = 15   # S·ªë khung h√¨nh li√™n ti·∫øp ƒë·ªÉ k√≠ch ho·∫°t
nod_flag_counter = 0

prev_time_fps = 0      # ƒê·ªÉ t√≠nh FPS

# --- Kh·ªüi ƒë·ªông Camera ---
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("[L·ªñI] Kh√¥ng th·ªÉ m·ªü webcam. Vui l√≤ng ki·ªÉm tra camera c·ªßa b·∫°n.")
    sys.exit()

print("üì∏ Nh·∫•n 'q' ƒë·ªÉ tho√°t...")

while True:
    ret, frame = cap.read()
    if not ret:
        print("[INFO] K·∫øt th√∫c lu·ªìng video.")
        break

    # L·∫•y k√≠ch th∆∞·ªõc khung h√¨nh
    h, w, _ = frame.shape
    if h == 0 or w == 0: continue

    frame = cv2.flip(frame, 1) # L·∫≠t ngang (nh∆∞ g∆∞∆°ng)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # X·ª≠ l√Ω frame v·ªõi Mediapipe
    results = face_mesh.process(rgb_frame)

    is_drowsy_trigger = False # Bi·∫øn ki·ªÉm so√°t vi·ªác k√≠ch ho·∫°t c·∫£nh b√°o

    if results.multi_face_landmarks:
        # Ch·ªâ l·∫•y khu√¥n m·∫∑t ƒë·∫ßu ti√™n
        face_landmarks = results.multi_face_landmarks[0]
        lm = face_landmarks.landmark
        
        # H√†m ti·ªán √≠ch ƒë·ªÉ l·∫•y t·ªça ƒë·ªô pixel (x, y)
        def get_coords(index):
            return (int(lm[index].x * w), int(lm[index].y * h))

        # --- 1. T√çNH TO√ÅN NH·∫ÆM M·∫ÆT (EAR) ---
        left_eye_coords = [get_coords(i) for i in LEFT_EYE]
        right_eye_coords = [get_coords(i) for i in RIGHT_EYE]
        
        # V·∫Ω ƒëa gi√°c quanh m·∫Øt
        cv2.polylines(frame, [np.array(left_eye_coords, dtype=np.int32)], True, (0,255,0), 1)
        cv2.polylines(frame, [np.array(right_eye_coords, dtype=np.int32)], True, (0,255,0), 1)

        leftEAR = eye_aspect_ratio(left_eye_coords)
        rightEAR = eye_aspect_ratio(right_eye_coords)
        ear = (leftEAR + rightEAR) / 2.0
        cv2.putText(frame, f"EAR: {ear:.2f}", (w - 150, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        # --- 2. T√çNH TO√ÅN NG√ÅP (MAR) ---
        # L·∫•y 4 ƒëi·ªÉm: m√¥i tr√™n (13), m√¥i d∆∞·ªõi (14), m√©p tr√°i (61), m√©p ph·∫£i (291)
        mouth_coords = [get_coords(13), get_coords(14), get_coords(61), get_coords(291)]
        mar = mouth_aspect_ratio(mouth_coords)
        cv2.putText(frame, f"MAR: {mar:.2f}", (w - 150, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        # --- TH√äM M·ªöI: V·∫Ω ƒë∆∞·ªùng vi·ªÅn mi·ªáng (gi·ªëng code 1) ---
        mouth_outline_coords = [get_coords(i) for i in MOUTH_OUTLINE]
        mouthHull = cv2.convexHull(np.array(mouth_outline_coords, dtype=np.int32))
        # V·∫Ω ƒë∆∞·ªùng vi·ªÅn m√†u v√†ng (BGR: 0, 255, 255)
        cv2.drawContours(frame, [mouthHull], -1, (0, 255, 255), 1)

        # --- 3. T√çNH TO√ÅN G·∫¨T ƒê·∫¶U (HEAD POSE - PITCH) ---
        # L·∫•y c√°c ƒëi·ªÉm 2D tr√™n ·∫£nh
        image_points = np.array([
            get_coords(1),    # M≈©i
            get_coords(199),  # C·∫±m
            get_coords(33),   # G√≥c m·∫Øt tr√°i
            get_coords(263),  # G√≥c m·∫Øt ph·∫£i
            get_coords(61),   # M√©p tr√°i
            get_coords(291)   # M√©p ph·∫£i
        ], dtype="double")
        
        # M√¥ h√¨nh 3D (t·ªça ƒë·ªô chu·∫©n - kh√¥ng c·∫ßn ch√≠nh x√°c tuy·ªát ƒë·ªëi)
        model_points = np.array([
            (0.0, 0.0, 0.0),      # M≈©i
            (0.0, -330.0, -65.0), # C·∫±m
            (-225.0, 170.0, -135.0), # G√≥c m·∫Øt tr√°i
            (225.0, 170.0, -135.0),  # G√≥c m·∫Øt ph·∫£i
            (-150.0, -150.0, -125.0), # M√©p tr√°i
            (150.0, -150.0, -125.0)   # M√©p ph·∫£i
        ])
        
        # Th√¥ng s·ªë camera (gi·∫£ ƒë·ªãnh)
        focal_length = w
        center = (w / 2, h / 2)
        camera_matrix = np.array(
            [[focal_length, 0, center[0]],
             [0, focal_length, center[1]],
             [0, 0, 1]], dtype="double"
        )
        dist_coeffs = np.zeros((4, 1)) # Kh√¥ng c√≥ m√©o ·ªëng k√≠nh

        # Gi·∫£i PnP ƒë·ªÉ t√¨m t∆∞ th·∫ø
        (success, rotation_vector, _) = cv2.solvePnP(
            model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE
        )

        pitch = 0.0
        if success:
            (rotation_matrix, _) = cv2.Rodrigues(rotation_vector)
            # Ph√¢n r√£ ma tr·∫≠n chi·∫øu ƒë·ªÉ l·∫•y g√≥c Euler
            (_, _, _, _, _, _, euler_angles) = cv2.decomposeProjectionMatrix(
                np.hstack((rotation_matrix, np.zeros((3, 1))))
            )
            # S·ª¨A L·ªñI: euler_angles[0] l√† m·ªôt m·∫£ng (v√≠ d·ª•: [20.5]).
            # Ch√∫ng ta c·∫ßn l·∫•y gi√° tr·ªã float b√™n trong n√≥ b·∫±ng [0][0]
            # tr∆∞·ªõc khi format b·∫±ng f-string ".:2f".
            pitch = euler_angles[0][0]
            cv2.putText(frame, f"Pitch: {pitch:.2f}", (w - 150, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # --- LOGIC C·ªú (FLAG) ---
        
        # C·ªù nh·∫Øm m·∫Øt
        if ear < EAR_THRESH:
            eye_flag_counter += 1
        else:
            eye_flag_counter = 0
        
        # C·ªù ng√°p
        if mar > MAR_THRESH:
            yawn_flag_counter += 1
        else:
            yawn_flag_counter = 0
        
        # C·ªù g·∫≠t ƒë·∫ßu
        if pitch > NOD_PITCH_THRESH:
            nod_flag_counter += 1
        else:
            nod_flag_counter = 0

        # --- KI·ªÇM TRA K√çCH HO·∫†T C·∫¢NH B√ÅO ---
        if eye_flag_counter >= EAR_FRAME_CHECK:
            is_drowsy_trigger = True
            cv2.putText(frame, "MAT NHAM", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        if yawn_flag_counter >= YAWN_FRAME_CHECK:
            is_drowsy_trigger = True
            cv2.putText(frame, "DANG NGAP", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            yawn_flag_counter = 0 # Reset ƒë·ªÉ ph√°t hi·ªán ng√°p l·∫ßn n·ªØa
        
        if nod_flag_counter >= NOD_FRAME_CHECK:
            is_drowsy_trigger = True
            cv2.putText(frame, "GAT DAU", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            nod_flag_counter = 0 # Reset

    else:
        # N·∫øu kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t, reset t·∫•t c·∫£ c√°c c·ªù
        eye_flag_counter = 0
        yawn_flag_counter = 0
        nod_flag_counter = 0

    # --- QU·∫¢N L√ù C·∫¢NH B√ÅO (NGO√ÄI V√íNG L·∫∂P LANDMARKS) ---
    
    if is_drowsy_trigger:
        cv2.putText(frame, "!!! CANH BAO NGU GAT !!!", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        # B·∫Øt ƒë·∫ßu ph√°t √¢m thanh (n·∫øu t·ªáp t·ªìn t·∫°i)
        if os.path.exists(SOUND_FILE):
            trigger_alert(start_alert=True)
    else:
        # D·ª´ng √¢m thanh (ch·ªâ khi m·∫Øt ƒë√£ m·ªü l·∫°i V√Ä kh√¥ng c√≥ c·ªù n√†o kh√°c)
        # Ch√∫ng ta d√πng `ear` t·ª´ v√≤ng l·∫∑p tr∆∞·ªõc, ho·∫∑c reset n·∫øu kh√¥ng c√≥ m·∫∑t
        ear_check = locals().get('ear', 0.0) # L·∫•y gi√° tr·ªã 'ear' n·∫øu t·ªìn t·∫°i
        if ear_check >= EAR_THRESH and eye_flag_counter == 0:
             trigger_alert(start_alert=False)

    # T√çNH FPS
    curr_time_fps = time.time()
    if curr_time_fps != prev_time_fps:
        fps = 1 / (curr_time_fps - prev_time_fps)
        prev_time_fps = curr_time_fps
        cv2.putText(frame, f"FPS: {int(fps)}", (w - 150, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

    # Hi·ªÉn th·ªã frame
    cv2.imshow("He thong Phat hien Ngu gat (Mediapipe)", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# --- D·ªåN D·∫∏P TR∆Ø·ªöC KHI THO√ÅT ---
print("[INFO] ƒêang d·ªçn d·∫πp v√† tho√°t...")
trigger_alert(start_alert=False) # ƒê·∫£m b·∫£o lu·ªìng √¢m thanh ƒë√£ t·∫Øt
cap.release()
cv2.destroyAllWindows()
face_mesh.close()